{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512cedd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:758: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 0.943717277486911\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        75\n",
      "           1       1.00      1.00      1.00        59\n",
      "           2       0.98      0.98      0.98        58\n",
      "           3       1.00      1.00      1.00        56\n",
      "           4       1.00      1.00      1.00        78\n",
      "           5       0.96      0.88      0.92        59\n",
      "           6       1.00      0.97      0.98        66\n",
      "           7       0.84      0.83      0.83        75\n",
      "           8       0.85      0.85      0.85        54\n",
      "           9       1.00      1.00      1.00        67\n",
      "          10       0.97      1.00      0.98        61\n",
      "          11       0.75      0.79      0.77        56\n",
      "\n",
      "    accuracy                           0.94       764\n",
      "   macro avg       0.94      0.94      0.94       764\n",
      "weighted avg       0.94      0.94      0.94       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix,  accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Data preprocessing function\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocesses a DataFrame by applying transformations to specific columns.\n",
    "    - Converts stringified sequences into Python lists of integers or floats.\n",
    "    - Converts approximate latitude and longitude values into integers.\n",
    "    - Converts boolean sequences into binary values (0 or 1).\n",
    "    - Drops unnecessary columns after processing.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame to be processed.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The processed DataFrame with transformed columns.\n",
    "    \"\"\"\n",
    "    # Convert stringified sequences to lists of integers or floats using eval\n",
    "    df[\"vehicles_sequence\"] = df[\"vehicles_sequence\"].apply(eval)\n",
    "    df[\"events_sequence\"] = df[\"events_sequence\"].apply(eval)\n",
    "    df[\"seconds_to_incident_sequence\"] = df[\"seconds_to_incident_sequence\"].apply(eval)\n",
    "    df[\"train_kph_sequence\"] = df[\"train_kph_sequence\"].apply(eval)\n",
    "\n",
    "    # Convert approx_lat and approx_lon to integers (handling potential NaN values)\n",
    "    df[\"approx_lat\"] = (\n",
    "        pd.to_numeric(df[\"approx_lat\"], errors=\"coerce\").fillna(np.nan).astype(int)\n",
    "    )\n",
    "    df[\"approx_lon\"] = (\n",
    "        pd.to_numeric(df[\"approx_lon\"], errors=\"coerce\").fillna(np.nan).astype(int)\n",
    "    )\n",
    "\n",
    "    # Convert boolean sequences (represented as strings) into lists of 0s and 1s\n",
    "    df[\"dj_ac_state_sequence\"] = df[\"dj_ac_state_sequence\"].apply(\n",
    "        lambda x: [1 if val else 0 for val in eval(x)]\n",
    "    )\n",
    "    df[\"dj_dc_state_sequence\"] = df[\"dj_dc_state_sequence\"].apply(\n",
    "        lambda x: [1 if val else 0 for val in eval(x)]\n",
    "    )\n",
    "\n",
    "    # Drop the approx_lat and approx_lon columns after processing\n",
    "    df.drop(columns=[\"approx_lat\", \"approx_lon\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to process the 'seconds_to_incident_sequence' column\n",
    "def process_seconds_sequence(data):\n",
    "    \"\"\"\n",
    "    Processes the 'seconds_to_incident_sequence' column in a DataFrame by extracting numerical\n",
    "    sequences, calculating summary statistics, and performing time gap analysis.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame containing the 'seconds_to_incident_sequence' column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with additional columns for summary statistics\n",
    "                  and time gap analysis.\n",
    "    \"\"\"\n",
    "    # Convert the 'seconds_to_incident_sequence' column from string format to lists of floats\n",
    "    seconds_sequence = (\n",
    "        data[\"seconds_to_incident_sequence\"]\n",
    "        .astype(str)\n",
    "        .apply(\n",
    "            lambda x: [float(i) for i in x.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Calculate summary statistics for the sequences\n",
    "    data[\"seconds_mean\"] = seconds_sequence.apply(np.mean)\n",
    "    data[\"seconds_std\"] = seconds_sequence.apply(np.std)\n",
    "    data[\"seconds_max\"] = seconds_sequence.apply(np.max)\n",
    "    data[\"seconds_min\"] = seconds_sequence.apply(np.min)\n",
    "    data[\"seconds_sum\"] = seconds_sequence.apply(np.sum)\n",
    "\n",
    "    # Perform time gap analysis by calculating differences between consecutive elements\n",
    "    data[\"seconds_gap_mean\"] = seconds_sequence.apply(\n",
    "        lambda x: np.mean(np.diff(x)) if len(x) > 1 else 0\n",
    "    )\n",
    "    data[\"seconds_gap_std\"] = seconds_sequence.apply(\n",
    "        lambda x: np.std(np.diff(x)) if len(x) > 1 else 0\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Optuna-integrated function to process event sequences into TF-IDF features and merge with the original DataFrame\n",
    "def process_event_sequences_to_tfidf_and_merge(\n",
    "    df, column_name, max_df=0.9841170198383609, min_df=9\n",
    "):\n",
    "    \"\"\"\n",
    "    Transforms a specified column of event sequences into TF-IDF features and merges these features\n",
    "    back into the original DataFrame. This function is intended for use with Optuna for hyperparameter optimization.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing event sequences in the specified column.\n",
    "    column_name (str): The name of the column to be processed (event sequences).\n",
    "    max_df (float, optional): The maximum document frequency for the TF-IDF Vectorizer.\n",
    "                              Terms appearing in more than this proportion of documents will be ignored.\n",
    "                              Default is 0.9841170198383609.\n",
    "    min_df (int, optional): The minimum document frequency for the TF-IDF Vectorizer.\n",
    "                            Terms appearing in fewer than this number of documents will be ignored.\n",
    "                            Default is 9.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with TF-IDF features merged into it.\n",
    "    \"\"\"\n",
    "    # Convert the event sequences in the column to strings, where each sequence is joined by spaces\n",
    "    docs = df[column_name].apply(lambda x: \" \".join(str(w) for w in x))\n",
    "\n",
    "    # Initialize the TF-IDF Vectorizer with specified max_df and min_df parameters\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df)\n",
    "\n",
    "    # Fit the vectorizer to the document strings and transform them into a sparse matrix\n",
    "    tfidf = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # Convert the resulting sparse matrix into a DataFrame, preserving column names and index\n",
    "    tfidf_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "        tfidf, columns=vectorizer.get_feature_names_out(), index=df.index\n",
    "    )\n",
    "\n",
    "    # Merge the original DataFrame with the new TF-IDF DataFrame along the columns\n",
    "    merged_df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Feature engineering function\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Performs feature engineering on the input DataFrame by generating new features\n",
    "    from existing columns related to vehicles, events, speed, and incident duration.\n",
    "    Additional processing includes integrating statistical features and TF-IDF transformation.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing sequences and incident-related data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with new engineered features and merged TF-IDF columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the total number of vehicles involved in each incident\n",
    "    df[\"vehicle_count\"] = df[\"vehicles_sequence\"].apply(len)\n",
    "\n",
    "    # Compute the number of unique vehicles involved in each incident\n",
    "    df[\"vehicles_unique_count\"] = df[\"vehicles_sequence\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "    # Compute the total number of events associated with each incident\n",
    "    df[\"event_count\"] = df[\"events_sequence\"].apply(len)\n",
    "\n",
    "    # Compute the number of unique events associated with each incident\n",
    "    df[\"events_unique_count\"] = df[\"events_sequence\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "    # Calculate the mean speed from the train's speed sequence\n",
    "    df[\"mean_speed\"] = df[\"train_kph_sequence\"].apply(np.mean)\n",
    "\n",
    "    # Calculate the total duration of the incident from the sequence of timestamps\n",
    "    df[\"incident_duration\"] = df[\"seconds_to_incident_sequence\"].apply(\n",
    "        lambda x: max(x) - min(x) if len(x) > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Process the 'seconds_to_incident_sequence' column to extract statistical features\n",
    "    df = process_seconds_sequence(df)\n",
    "\n",
    "    # Calculate additional statistical features for the train's speed sequence\n",
    "    df[\"max_speed\"] = df[\"train_kph_sequence\"].apply(np.max)\n",
    "    df[\"min_speed\"] = df[\"train_kph_sequence\"].apply(np.min)\n",
    "    df[\"std_speed\"] = df[\"train_kph_sequence\"].apply(np.std)\n",
    "\n",
    "    # Calculate the average number of events per vehicle involved in the incident\n",
    "    df[\"event_count_per_vehicle\"] = df[\"event_count\"] / df[\"vehicle_count\"]\n",
    "\n",
    "    # Calculate the average incident duration per vehicle involved\n",
    "    df[\"incident_duration_per_vehicle\"] = df[\"incident_duration\"] / df[\"vehicle_count\"]\n",
    "\n",
    "    # Compute the difference between the maximum and minimum speed for incidents with multiple vehicles\n",
    "    df[\"max_speed_diff\"] = df.apply(\n",
    "        lambda row: (\n",
    "            np.max(row[\"train_kph_sequence\"]) - np.min(row[\"train_kph_sequence\"])\n",
    "        )\n",
    "        if len(row[\"vehicles_sequence\"]) > 1\n",
    "        else 0,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Apply TF-IDF processing to the 'events_sequence' column and merge the results with the DataFrame\n",
    "    df = process_event_sequences_to_tfidf_and_merge(df, column_name=\"events_sequence\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def stacking_classification(X, y):\n",
    "       # Apply SMOTE (Synthetic Minority Oversampling Technique) for class imbalance handling\n",
    "     # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    \n",
    "    smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "    X_res, y_res = smote.fit_resample(X, y_encoded)\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_res, y_res, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Base classifiers\n",
    "    base_classifiers = [\n",
    "        ('rf', make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100))),\n",
    "        ('gb', make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=100))),\n",
    "        ('mlp', make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000)))\n",
    "    ]\n",
    "    \n",
    "    # Meta-classifier\n",
    "    meta_classifier = LogisticRegression()\n",
    "    \n",
    "    # Stacking Classifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_classifiers,\n",
    "        final_estimator=meta_classifier,\n",
    "        cv=5  # 5-fold cross-validation\n",
    "    )\n",
    "    \n",
    "    # Fit and predict\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    stacking_predictions = stacking_clf.predict(X_test)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    cm = confusion_matrix(y_test, stacking_predictions)\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"magma\",\n",
    "        xticklabels=le.classes_,\n",
    "        yticklabels=le.classes_,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'stacking_model': stacking_clf,\n",
    "        'predictions': stacking_predictions,\n",
    "        'accuracy': accuracy_score(y_test, stacking_predictions),\n",
    "        'classification_report': classification_report(y_test, stacking_predictions)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Main script execution\n",
    "    file_path = \"sncb_data_challenge.csv\"  # Adjust this path as needed\n",
    "    df = pd.read_csv(file_path, sep=\";\", index_col=0)\n",
    "\n",
    "    # Preprocess data\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Perform feature engineering\n",
    "    df = feature_engineering(df)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    y = df[\"incident_type\"]\n",
    "    X = df.loc[:, \"vehicle_count\":]\n",
    "\n",
    "     # Perform stacking classification\n",
    "    results = stacking_classification(X, y)\n",
    "        \n",
    "    print(\"Stacking Classifier Accuracy:\", results['accuracy'])\n",
    "    print(\"\\nClassification Report:\\n\", results['classification_report'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc7f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Data preprocessing function\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocesses a DataFrame by applying transformations to specific columns.\n",
    "    - Converts stringified sequences into Python lists of integers or floats.\n",
    "    - Converts approximate latitude and longitude values into integers.\n",
    "    - Converts boolean sequences into binary values (0 or 1).\n",
    "    - Drops unnecessary columns after processing.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame to be processed.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The processed DataFrame with transformed columns.\n",
    "    \"\"\"\n",
    "    # Convert stringified sequences to lists of integers or floats using eval\n",
    "    df[\"vehicles_sequence\"] = df[\"vehicles_sequence\"].apply(eval)\n",
    "    df[\"events_sequence\"] = df[\"events_sequence\"].apply(eval)\n",
    "    df[\"seconds_to_incident_sequence\"] = df[\"seconds_to_incident_sequence\"].apply(eval)\n",
    "    df[\"train_kph_sequence\"] = df[\"train_kph_sequence\"].apply(eval)\n",
    "\n",
    "    # Convert approx_lat and approx_lon to integers (handling potential NaN values)\n",
    "    df[\"approx_lat\"] = (\n",
    "        pd.to_numeric(df[\"approx_lat\"], errors=\"coerce\").fillna(np.nan).astype(int)\n",
    "    )\n",
    "    df[\"approx_lon\"] = (\n",
    "        pd.to_numeric(df[\"approx_lon\"], errors=\"coerce\").fillna(np.nan).astype(int)\n",
    "    )\n",
    "\n",
    "    # Convert boolean sequences (represented as strings) into lists of 0s and 1s\n",
    "    df[\"dj_ac_state_sequence\"] = df[\"dj_ac_state_sequence\"].apply(\n",
    "        lambda x: [1 if val else 0 for val in eval(x)]\n",
    "    )\n",
    "    df[\"dj_dc_state_sequence\"] = df[\"dj_dc_state_sequence\"].apply(\n",
    "        lambda x: [1 if val else 0 for val in eval(x)]\n",
    "    )\n",
    "\n",
    "    # Drop the approx_lat and approx_lon columns after processing\n",
    "    df.drop(columns=[\"approx_lat\", \"approx_lon\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to process the 'seconds_to_incident_sequence' column\n",
    "def process_seconds_sequence(data):\n",
    "    \"\"\"\n",
    "    Processes the 'seconds_to_incident_sequence' column in a DataFrame by extracting numerical\n",
    "    sequences, calculating summary statistics, and performing time gap analysis.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input DataFrame containing the 'seconds_to_incident_sequence' column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with additional columns for summary statistics\n",
    "                  and time gap analysis.\n",
    "    \"\"\"\n",
    "    # Convert the 'seconds_to_incident_sequence' column from string format to lists of floats\n",
    "    seconds_sequence = (\n",
    "        data[\"seconds_to_incident_sequence\"]\n",
    "        .astype(str)\n",
    "        .apply(\n",
    "            lambda x: [float(i) for i in x.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Calculate summary statistics for the sequences\n",
    "    data[\"seconds_mean\"] = seconds_sequence.apply(np.mean)\n",
    "    data[\"seconds_std\"] = seconds_sequence.apply(np.std)\n",
    "    data[\"seconds_max\"] = seconds_sequence.apply(np.max)\n",
    "    data[\"seconds_min\"] = seconds_sequence.apply(np.min)\n",
    "    data[\"seconds_sum\"] = seconds_sequence.apply(np.sum)\n",
    "\n",
    "    # Perform time gap analysis by calculating differences between consecutive elements\n",
    "    data[\"seconds_gap_mean\"] = seconds_sequence.apply(\n",
    "        lambda x: np.mean(np.diff(x)) if len(x) > 1 else 0\n",
    "    )\n",
    "    data[\"seconds_gap_std\"] = seconds_sequence.apply(\n",
    "        lambda x: np.std(np.diff(x)) if len(x) > 1 else 0\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Optuna-integrated function to process event sequences into TF-IDF features and merge with the original DataFrame\n",
    "def process_event_sequences_to_tfidf_and_merge(\n",
    "    df, column_name, max_df=0.9841170198383609, min_df=9\n",
    "):\n",
    "    \"\"\"\n",
    "    Transforms a specified column of event sequences into TF-IDF features and merges these features\n",
    "    back into the original DataFrame. This function is intended for use with Optuna for hyperparameter optimization.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing event sequences in the specified column.\n",
    "    column_name (str): The name of the column to be processed (event sequences).\n",
    "    max_df (float, optional): The maximum document frequency for the TF-IDF Vectorizer.\n",
    "                              Terms appearing in more than this proportion of documents will be ignored.\n",
    "                              Default is 0.9841170198383609.\n",
    "    min_df (int, optional): The minimum document frequency for the TF-IDF Vectorizer.\n",
    "                            Terms appearing in fewer than this number of documents will be ignored.\n",
    "                            Default is 9.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with TF-IDF features merged into it.\n",
    "    \"\"\"\n",
    "    # Convert the event sequences in the column to strings, where each sequence is joined by spaces\n",
    "    docs = df[column_name].apply(lambda x: \" \".join(str(w) for w in x))\n",
    "\n",
    "    # Initialize the TF-IDF Vectorizer with specified max_df and min_df parameters\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df)\n",
    "\n",
    "    # Fit the vectorizer to the document strings and transform them into a sparse matrix\n",
    "    tfidf = vectorizer.fit_transform(docs)\n",
    "\n",
    "    # Convert the resulting sparse matrix into a DataFrame, preserving column names and index\n",
    "    tfidf_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "        tfidf, columns=vectorizer.get_feature_names_out(), index=df.index\n",
    "    )\n",
    "\n",
    "    # Merge the original DataFrame with the new TF-IDF DataFrame along the columns\n",
    "    merged_df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Feature engineering function\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Performs feature engineering on the input DataFrame by generating new features\n",
    "    from existing columns related to vehicles, events, speed, and incident duration.\n",
    "    Additional processing includes integrating statistical features and TF-IDF transformation.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing sequences and incident-related data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with new engineered features and merged TF-IDF columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the total number of vehicles involved in each incident\n",
    "    df[\"vehicle_count\"] = df[\"vehicles_sequence\"].apply(len)\n",
    "\n",
    "    # Compute the number of unique vehicles involved in each incident\n",
    "    df[\"vehicles_unique_count\"] = df[\"vehicles_sequence\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "    # Compute the total number of events associated with each incident\n",
    "    df[\"event_count\"] = df[\"events_sequence\"].apply(len)\n",
    "\n",
    "    # Compute the number of unique events associated with each incident\n",
    "    df[\"events_unique_count\"] = df[\"events_sequence\"].apply(lambda x: len(set(x)))\n",
    "\n",
    "    # Calculate the mean speed from the train's speed sequence\n",
    "    df[\"mean_speed\"] = df[\"train_kph_sequence\"].apply(np.mean)\n",
    "\n",
    "    # Calculate the total duration of the incident from the sequence of timestamps\n",
    "    df[\"incident_duration\"] = df[\"seconds_to_incident_sequence\"].apply(\n",
    "        lambda x: max(x) - min(x) if len(x) > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Process the 'seconds_to_incident_sequence' column to extract statistical features\n",
    "    df = process_seconds_sequence(df)\n",
    "\n",
    "    # Calculate additional statistical features for the train's speed sequence\n",
    "    df[\"max_speed\"] = df[\"train_kph_sequence\"].apply(np.max)\n",
    "    df[\"min_speed\"] = df[\"train_kph_sequence\"].apply(np.min)\n",
    "    df[\"std_speed\"] = df[\"train_kph_sequence\"].apply(np.std)\n",
    "\n",
    "    # Calculate the average number of events per vehicle involved in the incident\n",
    "    df[\"event_count_per_vehicle\"] = df[\"event_count\"] / df[\"vehicle_count\"]\n",
    "\n",
    "    # Calculate the average incident duration per vehicle involved\n",
    "    df[\"incident_duration_per_vehicle\"] = df[\"incident_duration\"] / df[\"vehicle_count\"]\n",
    "\n",
    "    # Compute the difference between the maximum and minimum speed for incidents with multiple vehicles\n",
    "    df[\"max_speed_diff\"] = df.apply(\n",
    "        lambda row: (\n",
    "            np.max(row[\"train_kph_sequence\"]) - np.min(row[\"train_kph_sequence\"])\n",
    "        )\n",
    "        if len(row[\"vehicles_sequence\"]) > 1\n",
    "        else 0,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Apply TF-IDF processing to the 'events_sequence' column and merge the results with the DataFrame\n",
    "    df = process_event_sequences_to_tfidf_and_merge(df, column_name=\"events_sequence\")\n",
    "\n",
    "    return df\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def stacking_classification(X, y):\n",
    "      # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Apply DBSCAN for clustering\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    dbscan.fit(X_train_scaled)\n",
    "    \n",
    "    # Extract the cluster labels and add as a new feature\n",
    "    X_train['cluster'] = dbscan.labels_\n",
    "    X_test['cluster'] = dbscan.fit_predict(X_test_scaled)\n",
    "    \n",
    "    # Base classifiers with hyperparameters for tuning\n",
    "    base_classifiers = [\n",
    "        ('rf', make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=200, max_depth=10, class_weight='balanced'))),\n",
    "        ('gb', make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=5))),\n",
    "        ('mlp', make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)))\n",
    "    ]\n",
    "\n",
    "    # Meta-classifier (using a stronger model)\n",
    "    meta_classifier = LogisticRegression(C=10, solver='liblinear')\n",
    "\n",
    "    \n",
    "    # Stacking Classifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_classifiers,\n",
    "        final_estimator=meta_classifier,\n",
    "        cv=5  # 5-fold cross-validation\n",
    "    )\n",
    "    \n",
    "    # Fit and predict\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    stacking_predictions = stacking_clf.predict(X_test)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    cm = confusion_matrix(y_test, stacking_predictions)\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"magma\",\n",
    "        xticklabels=le.classes_,\n",
    "        yticklabels=le.classes_,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'stacking_model': stacking_clf,\n",
    "        'predictions': stacking_predictions,\n",
    "        'accuracy': accuracy_score(y_test, stacking_predictions),\n",
    "        'classification_report': classification_report(y_test, stacking_predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def apriori_with_stacking_classification(X, y, min_support=0.1, min_threshold=0.5):\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Apply Apriori to find frequent itemsets in the training data\n",
    "    # Convert the training data into a binary format (e.g., if a feature value is greater than a threshold, it's 1)\n",
    "    X_train_binary = X_train > X_train.mean()\n",
    "    \n",
    "    # Find frequent itemsets with Apriori\n",
    "    frequent_itemsets = apriori(X_train_binary, min_support=min_support, use_colnames=True)\n",
    "    \n",
    "    # Generate association rules from the frequent itemsets\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=min_threshold)\n",
    "    \n",
    "    # Extract the antecedents and consequents of the rules and use them as features\n",
    "    rule_features = []\n",
    "    for _, rule in rules.iterrows():\n",
    "        for item in rule['antecedents']:\n",
    "            rule_features.append(str(item))  # Extract the rule features\n",
    "    \n",
    "    # Create new columns for each rule in the original data\n",
    "    for rule_feature in rule_features:\n",
    "        X_train_scaled[rule_feature] = X_train_binary.apply(lambda row: 1 if row[rule_feature] == 1 else 0, axis=1)\n",
    "        X_test_scaled[rule_feature] = X_test_binary.apply(lambda row: 1 if row[rule_feature] == 1 else 0, axis=1)\n",
    "    \n",
    "    # Base classifiers with hyperparameters for tuning\n",
    "    base_classifiers = [\n",
    "        ('rf', make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=200, max_depth=10, class_weight='balanced'))),\n",
    "        ('gb', make_pipeline(StandardScaler(), GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=5))),\n",
    "        ('mlp', make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)))\n",
    "    ]\n",
    "\n",
    "    # Meta-classifier (Logistic Regression)\n",
    "    meta_classifier = LogisticRegression(C=10, solver='liblinear')\n",
    "    \n",
    "    # Stacking Classifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_classifiers,\n",
    "        final_estimator=meta_classifier,\n",
    "        cv=5  # 5-fold cross-validation\n",
    "    )\n",
    "    \n",
    "    # Fit and predict\n",
    "    stacking_clf.fit(X_train_scaled, y_train)\n",
    "    stacking_predictions = stacking_clf.predict(X_test_scaled)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    cm = confusion_matrix(y_test, stacking_predictions)\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"magma\",\n",
    "        xticklabels=le.classes_,\n",
    "        yticklabels=le.classes_,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'stacking_model': stacking_clf,\n",
    "        'predictions': stacking_predictions,\n",
    "        'accuracy': accuracy_score(y_test, stacking_predictions),\n",
    "        'classification_report': classification_report(y_test, stacking_predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Main script execution\n",
    "    file_path = \"sncb_data_challenge.csv\"  # Adjust this path as needed\n",
    "    df = pd.read_csv(file_path, sep=\";\", index_col=0)\n",
    "\n",
    "    # Preprocess data\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Perform feature engineering\n",
    "    df = feature_engineering(df)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    y = df[\"incident_type\"]\n",
    "    X = df.loc[:, \"vehicle_count\":]\n",
    "\n",
    "     # Perform stacking classification\n",
    "    results = apriori_with_stacking_classification(X, y)\n",
    "        \n",
    "    print(\"Stacking Classifier Accuracy:\", results['accuracy'])\n",
    "    print(\"\\nClassification Report:\\n\", results['classification_report'])\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200bc2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 14 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 16 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     transactions\u001b[38;5;241m.\u001b[39mappend(transaction)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Apply ECLAT\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m eclat \u001b[38;5;241m=\u001b[39m \u001b[43mECLAT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m frequent_itemsets, _ \u001b[38;5;241m=\u001b[39m eclat\u001b[38;5;241m.\u001b[39mfit(min_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, min_combination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_combination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Convert frequent patterns into features\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyECLAT/pyECLAT.py:62\u001b[0m, in \u001b[0;36mECLAT.__init__\u001b[0;34m(self, data, verbose)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muniq_ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 62\u001b[0m \u001b[43mECLAT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUnique\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_bin \u001b[38;5;241m=\u001b[39m ECLAT\u001b[38;5;241m.\u001b[39m_makeTable(\u001b[38;5;28mself\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyECLAT/pyECLAT.py:72\u001b[0m, in \u001b[0;36mECLAT._getUnique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getUnique\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Return a list with unique names of features\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     dif_atrib \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 72\u001b[0m     n_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_columns):\n\u001b[1;32m     74\u001b[0m         dif_atrib\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[:, column]\u001b[38;5;241m.\u001b[39munique()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from pyECLAT import ECLAT\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "    # Main script execution\n",
    "file_path = \"sncb_data_challenge.csv\"  # Adjust this path as needed\n",
    "df = pd.read_csv(file_path, sep=\";\", index_col=0)\n",
    "\n",
    "    # Preprocess data\n",
    "df = preprocess_data(df)\n",
    "\n",
    "    # Perform feature engineering\n",
    "df = feature_engineering(df)\n",
    "\n",
    "    # Train and evaluate models\n",
    "y = df[\"incident_type\"]\n",
    "X = df.loc[:, \"vehicle_count\":]\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Discretize features for ECLAT\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')  # Binning into 3 bins\n",
    "X_train_discretized = discretizer.fit_transform(X_train)\n",
    "\n",
    "# Convert to transactional format for ECLAT\n",
    "transactions = []\n",
    "for row in X_train_discretized:\n",
    "    transaction = [f\"feature_{i}_bin_{int(value)}\" for i, value in enumerate(row)]\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Apply ECLAT\n",
    "eclat = ECLAT(data=transactions, verbose=True)\n",
    "frequent_itemsets, _ = eclat.fit(min_support=0.3, min_combination=1, max_combination=3)\n",
    "\n",
    "# Convert frequent patterns into features\n",
    "frequent_patterns = list(frequent_itemsets.keys())\n",
    "X_train_patterns = pd.DataFrame(0, index=range(len(X_train)), columns=frequent_patterns)\n",
    "\n",
    "# Populate the new feature matrix with the presence of patterns\n",
    "for i, transaction in enumerate(transactions):\n",
    "    for pattern in frequent_patterns:\n",
    "        if set(pattern).issubset(set(transaction)):\n",
    "            X_train_patterns.loc[i, pattern] = 1\n",
    "\n",
    "# 5. Standardize the original dataset\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Combine original and pattern-based features\n",
    "X_train_combined = pd.concat([pd.DataFrame(X_train_scaled), X_train_patterns], axis=1)\n",
    "\n",
    "# Apply similar transformations to X_test\n",
    "X_test_discretized = discretizer.transform(X_test)\n",
    "transactions_test = []\n",
    "for row in X_test_discretized:\n",
    "    transaction = [f\"feature_{i}_bin_{int(value)}\" for i, value in enumerate(row)]\n",
    "    transactions_test.append(transaction)\n",
    "\n",
    "X_test_patterns = pd.DataFrame(0, index=range(len(X_test)), columns=frequent_patterns)\n",
    "for i, transaction in enumerate(transactions_test):\n",
    "    for pattern in frequent_patterns:\n",
    "        if set(pattern).issubset(set(transaction)):\n",
    "            X_test_patterns.loc[i, pattern] = 1\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_combined = pd.concat([pd.DataFrame(X_test_scaled), X_test_patterns], axis=1)\n",
    "\n",
    "# Train the stacking classifier\n",
    "base_classifiers = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, class_weight='balanced')),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=5)),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42))\n",
    "]\n",
    "meta_classifier = LogisticRegression(C=10, solver='liblinear')\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier, cv=5)\n",
    "stacking_clf.fit(X_train_combined, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "stacking_predictions = stacking_clf.predict(X_test_combined)\n",
    "cm = confusion_matrix(y_test, stacking_predictions)\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"magma\",\n",
    "    xticklabels=le.classes_,\n",
    "    yticklabels=le.classes_,\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Return results\n",
    "results = {\n",
    "    'stacking_model': stacking_clf,\n",
    "    'predictions': stacking_predictions,\n",
    "    'accuracy': accuracy_score(y_test, stacking_predictions),\n",
    "    'classification_report': classification_report(y_test, stacking_predictions)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84030783",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(file_path, sep=\";\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f843a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 14 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 16 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/Users/muhammadehsansiddique/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:877: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     transactions\u001b[38;5;241m.\u001b[39mappend(transaction)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Apply ECLAT\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m eclat \u001b[38;5;241m=\u001b[39m \u001b[43mECLAT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m frequent_itemsets, _ \u001b[38;5;241m=\u001b[39m eclat\u001b[38;5;241m.\u001b[39mfit(min_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, min_combination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_combination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Convert frequent patterns to binary features\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyECLAT/pyECLAT.py:62\u001b[0m, in \u001b[0;36mECLAT.__init__\u001b[0;34m(self, data, verbose)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muniq_ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 62\u001b[0m \u001b[43mECLAT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUnique\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_bin \u001b[38;5;241m=\u001b[39m ECLAT\u001b[38;5;241m.\u001b[39m_makeTable(\u001b[38;5;28mself\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyECLAT/pyECLAT.py:72\u001b[0m, in \u001b[0;36mECLAT._getUnique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getUnique\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Return a list with unique names of features\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     dif_atrib \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 72\u001b[0m     n_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_columns):\n\u001b[1;32m     74\u001b[0m         dif_atrib\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[:, column]\u001b[38;5;241m.\u001b[39munique()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from pyECLAT import ECLAT\n",
    "\n",
    "\n",
    "    # Preprocess data\n",
    "dataset = preprocess_data(dataset)\n",
    "\n",
    "    # Perform feature engineering\n",
    "dataset = feature_engineering(dataset)\n",
    "\n",
    "# Assume the dataset has a target column 'target' (replace 'target' with the actual name)\n",
    "y = dataset[\"incident_type\"]\n",
    "X = dataset.loc[:, \"vehicle_count\":]\n",
    "\n",
    "# X = dataset.drop(columns=[''])  # Replace 'target' with the actual target column name\n",
    "# y = dataset['target']\n",
    "\n",
    "# Discretize continuous features into bins (convert to categorical for ECLAT)\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "X_discretized = discretizer.fit_transform(X)\n",
    "\n",
    "# Convert to transactional format\n",
    "transactions = []\n",
    "for row in X_discretized:\n",
    "    transaction = [f\"feature_{i}_bin_{int(value)}\" for i, value in enumerate(row)]\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Apply ECLAT\n",
    "eclat = ECLAT(data=transactions, verbose=True)\n",
    "frequent_itemsets, _ = eclat.fit(min_support=0.3, min_combination=1, max_combination=3)\n",
    "\n",
    "# Convert frequent patterns to binary features\n",
    "frequent_patterns = list(frequent_itemsets.keys())\n",
    "X_patterns = pd.DataFrame(0, index=range(len(X_discretized)), columns=frequent_patterns)\n",
    "for i, transaction in enumerate(transactions):\n",
    "    for pattern in frequent_patterns:\n",
    "        if set(pattern).issubset(set(transaction)):\n",
    "            X_patterns.loc[i, pattern] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd9e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"featured_eng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d768dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
