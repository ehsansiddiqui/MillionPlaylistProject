{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aedd635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39092d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 03:57:55 WARN Utils: Your hostname, Muhammads-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.50.15.243 instead (on interface en0)\n",
      "24/12/14 03:57:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/14 03:57:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    " # Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Playlist Similarity\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5854c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('../data/processed/df_tracks.parquet', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab075ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2262292"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ebd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.limit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05b539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------------+--------------------+-----------+--------------------+--------------------+---+\n",
      "|          album_name|           album_uri|      artist_name|          artist_uri|duration_ms|          track_name|           track_uri|tid|\n",
      "+--------------------+--------------------+-----------------+--------------------+-----------+--------------------+--------------------+---+\n",
      "|The Times They Ar...|spotify:album:7DZ...|        Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|\n",
      "|Bringing It All B...|spotify:album:1lP...|        Bob Dylan|spotify:artist:74...|     330533|  Mr. Tambourine Man|spotify:track:3Rk...|  1|\n",
      "|The Best: Loggins...|spotify:album:5BW...|Loggins & Messina|spotify:artist:7e...|     254653|        Danny's Song|spotify:track:0ju...|  2|\n",
      "|The Freewheelin' ...|spotify:album:0o1...|        Bob Dylan|spotify:artist:74...|     412200|A Hard Rain's A-G...|spotify:track:7ny...|  3|\n",
      "|The Freewheelin' ...|spotify:album:0o1...|        Bob Dylan|spotify:artist:74...|     165426| Blowin' In the Wind|spotify:track:18G...|  4|\n",
      "| John Wesley Harding|spotify:album:2Kz...|        Bob Dylan|spotify:artist:74...|     177200| John Wesley Harding|spotify:track:0wf...|  5|\n",
      "| Blood On The Tracks|spotify:album:4WD...|        Bob Dylan|spotify:artist:74...|     288000|If You See Her, S...|spotify:track:3xN...|  6|\n",
      "|Bringing It All B...|spotify:album:1lP...|        Bob Dylan|spotify:artist:74...|     389800|Bob Dylan's 115th...|spotify:track:5ka...|  7|\n",
      "|Bringing It All B...|spotify:album:1lP...|        Bob Dylan|spotify:artist:74...|     171106|     Love Minus Zero|spotify:track:2FD...|  8|\n",
      "|       The Lumineers|spotify:album:5h7...|    The Lumineers|spotify:artist:16...|     165933|        Classy Girls|spotify:track:2Gb...|  9|\n",
      "|   Nashville Skyline|spotify:album:5WB...|        Bob Dylan|spotify:artist:74...|     220400|Girl from the Nor...|spotify:track:4K1...| 10|\n",
      "|The Blueprint 2 T...|spotify:album:6Zb...|            JAY Z|spotify:artist:3n...|     205560|  03' Bonnie & Clyde|spotify:track:73A...| 11|\n",
      "|                   8|spotify:album:6y3...|       Luis Fonsi|spotify:artist:4V...|     274213|   Que Quieres De Mi|spotify:track:3CP...| 12|\n",
      "|        Wicked Games|spotify:album:29O...|   Parra for Cuva|spotify:artist:23...|     358573|        Wicked Games|spotify:track:3mW...| 13|\n",
      "|       The Lumineers|spotify:album:0nR...|    The Lumineers|spotify:artist:16...|     307000|        Slow It Down|spotify:track:5OF...| 14|\n",
      "|            Greatest|spotify:album:684...|         Bee Gees|spotify:artist:1L...|     245200|How Deep Is Your ...|spotify:track:3aq...| 15|\n",
      "|     Cama Incendiada|spotify:album:36l...|             Man√°|spotify:artist:7o...|     272576|Mi verdad - feat....|spotify:track:3Ym...| 16|\n",
      "|Breakfast At Tiff...|spotify:album:53m...|    Henry Mancini|spotify:artist:2E...|     125585|Moon River(Vocal ...|spotify:track:5iG...| 17|\n",
      "|    Ultimate Sinatra|spotify:album:0Cx...|    Frank Sinatra|spotify:artist:1M...|     129813|I've Got The Worl...|spotify:track:4ts...| 18|\n",
      "|The Essential Elv...|spotify:album:3X3...|    Elvis Presley|spotify:artist:43...|     132013|A Little Less Con...|spotify:track:47y...| 19|\n",
      "+--------------------+--------------------+-----------------+--------------------+-----------+--------------------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a75b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Cartesian join to compute all pairwise comparisons\n",
    "pairwise_df = df.alias(\"a\").crossJoin(df.alias(\"b\")).filter(col(\"a.tid\") < col(\"b.tid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "541a22da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+-----------+--------------------+--------------------+---+--------------------+--------------------+-----------------+--------------------+-----------+--------------------+--------------------+---+\n",
      "|          album_name|           album_uri|artist_name|          artist_uri|duration_ms|          track_name|           track_uri|tid|          album_name|           album_uri|      artist_name|          artist_uri|duration_ms|          track_name|           track_uri|tid|\n",
      "+--------------------+--------------------+-----------+--------------------+-----------+--------------------+--------------------+---+--------------------+--------------------+-----------------+--------------------+-----------+--------------------+--------------------+---+\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|Bringing It All B...|spotify:album:1lP...|        Bob Dylan|spotify:artist:74...|     330533|  Mr. Tambourine Man|spotify:track:3Rk...|  1|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Best: Loggins...|spotify:album:5BW...|Loggins & Messina|spotify:artist:7e...|     254653|        Danny's Song|spotify:track:0ju...|  2|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Freewheelin' ...|spotify:album:0o1...|        Bob Dylan|spotify:artist:74...|     412200|A Hard Rain's A-G...|spotify:track:7ny...|  3|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Freewheelin' ...|spotify:album:0o1...|        Bob Dylan|spotify:artist:74...|     165426| Blowin' In the Wind|spotify:track:18G...|  4|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0| John Wesley Harding|spotify:album:2Kz...|        Bob Dylan|spotify:artist:74...|     177200| John Wesley Harding|spotify:track:0wf...|  5|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0| Blood On The Tracks|spotify:album:4WD...|        Bob Dylan|spotify:artist:74...|     288000|If You See Her, S...|spotify:track:3xN...|  6|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|Bringing It All B...|spotify:album:1lP...|        Bob Dylan|spotify:artist:74...|     389800|Bob Dylan's 115th...|spotify:track:5ka...|  7|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|Bringing It All B...|spotify:album:1lP...|        Bob Dylan|spotify:artist:74...|     171106|     Love Minus Zero|spotify:track:2FD...|  8|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|       The Lumineers|spotify:album:5h7...|    The Lumineers|spotify:artist:16...|     165933|        Classy Girls|spotify:track:2Gb...|  9|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|   Nashville Skyline|spotify:album:5WB...|        Bob Dylan|spotify:artist:74...|     220400|Girl from the Nor...|spotify:track:4K1...| 10|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Blueprint 2 T...|spotify:album:6Zb...|            JAY Z|spotify:artist:3n...|     205560|  03' Bonnie & Clyde|spotify:track:73A...| 11|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|                   8|spotify:album:6y3...|       Luis Fonsi|spotify:artist:4V...|     274213|   Que Quieres De Mi|spotify:track:3CP...| 12|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|        Wicked Games|spotify:album:29O...|   Parra for Cuva|spotify:artist:23...|     358573|        Wicked Games|spotify:track:3mW...| 13|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|       The Lumineers|spotify:album:0nR...|    The Lumineers|spotify:artist:16...|     307000|        Slow It Down|spotify:track:5OF...| 14|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|            Greatest|spotify:album:684...|         Bee Gees|spotify:artist:1L...|     245200|How Deep Is Your ...|spotify:track:3aq...| 15|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|     Cama Incendiada|spotify:album:36l...|             Man√°|spotify:artist:7o...|     272576|Mi verdad - feat....|spotify:track:3Ym...| 16|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|Breakfast At Tiff...|spotify:album:53m...|    Henry Mancini|spotify:artist:2E...|     125585|Moon River(Vocal ...|spotify:track:5iG...| 17|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|    Ultimate Sinatra|spotify:album:0Cx...|    Frank Sinatra|spotify:artist:1M...|     129813|I've Got The Worl...|spotify:track:4ts...| 18|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Essential Elv...|spotify:album:3X3...|    Elvis Presley|spotify:artist:43...|     132013|A Little Less Con...|spotify:track:47y...| 19|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|    For LP Fans Only|spotify:album:1NV...|    Elvis Presley|spotify:artist:43...|     115280|    That's All Right|spotify:track:0tH...| 20|\n",
      "+--------------------+--------------------+-----------+--------------------+-----------+--------------------+--------------------+---+--------------------+--------------------+-----------------+--------------------+-----------+--------------------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairwise_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76999bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity calculation\n",
    "def compute_similarity(df):\n",
    "    return df.withColumn(\n",
    "        \"artist_similarity\", when(col(\"a.artist_name\") == col(\"b.artist_name\"), lit(1.0)).otherwise(lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"album_similarity\", when(col(\"a.album_name\") == col(\"b.album_name\"), lit(1.0)).otherwise(lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"duration_diff\", abs(col(\"a.duration_ms\") - col(\"b.duration_ms\"))\n",
    "    ).withColumn(\n",
    "        \"duration_similarity\", 1 - (col(\"duration_diff\") / lit(500000))  # Scale duration diff to [0, 1]\n",
    "    ).withColumn(\n",
    "        \"similarity_score\",\n",
    "        (col(\"artist_similarity\") * 0.5) + (col(\"album_similarity\") * 0.3) + (col(\"duration_similarity\") * 0.2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c55a1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = compute_similarity(pairwise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d41f1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+-----------+--------------------+--------------------+---+--------------------+--------------------+-----------------+--------------------+-----------+--------------------+--------------------+---+-----------------+----------------+-------------+-------------------+-------------------+\n",
      "|          album_name|           album_uri|artist_name|          artist_uri|duration_ms|          track_name|           track_uri|tid|          album_name|           album_uri|      artist_name|          artist_uri|duration_ms|          track_name|           track_uri|tid|artist_similarity|album_similarity|duration_diff|duration_similarity|   similarity_score|\n",
      "+--------------------+--------------------+-----------+--------------------+-----------+--------------------+--------------------+---+--------------------+--------------------+-----------------+--------------------+-----------+--------------------+--------------------+---+-----------------+----------------+-------------+-------------------+-------------------+\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|Bringing It All B...|spotify:album:1lP...|        Bob Dylan|spotify:artist:74...|     330533|  Mr. Tambourine Man|spotify:track:3Rk...|  1|              1.0|             0.0|        53427|           0.893146|          0.6786292|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Best: Loggins...|spotify:album:5BW...|Loggins & Messina|spotify:artist:7e...|     254653|        Danny's Song|spotify:track:0ju...|  2|              0.0|             0.0|        22453|           0.955094|0.19101880000000002|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Freewheelin' ...|spotify:album:0o1...|        Bob Dylan|spotify:artist:74...|     412200|A Hard Rain's A-G...|spotify:track:7ny...|  3|              1.0|             0.0|       135094|           0.729812|          0.6459624|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Freewheelin' ...|spotify:album:0o1...|        Bob Dylan|spotify:artist:74...|     165426| Blowin' In the Wind|spotify:track:18G...|  4|              1.0|             0.0|       111680|            0.77664|           0.655328|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0| John Wesley Harding|spotify:album:2Kz...|        Bob Dylan|spotify:artist:74...|     177200| John Wesley Harding|spotify:track:0wf...|  5|              1.0|             0.0|        99906|           0.800188|          0.6600376|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0| Blood On The Tracks|spotify:album:4WD...|        Bob Dylan|spotify:artist:74...|     288000|If You See Her, S...|spotify:track:3xN...|  6|              1.0|             0.0|        10894|           0.978212|          0.6956424|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|Bringing It All B...|spotify:album:1lP...|        Bob Dylan|spotify:artist:74...|     389800|Bob Dylan's 115th...|spotify:track:5ka...|  7|              1.0|             0.0|       112694|           0.774612|          0.6549224|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|Bringing It All B...|spotify:album:1lP...|        Bob Dylan|spotify:artist:74...|     171106|     Love Minus Zero|spotify:track:2FD...|  8|              1.0|             0.0|       106000|              0.788|             0.6576|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|       The Lumineers|spotify:album:5h7...|    The Lumineers|spotify:artist:16...|     165933|        Classy Girls|spotify:track:2Gb...|  9|              0.0|             0.0|       111173| 0.7776540000000001|0.15553080000000002|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|   Nashville Skyline|spotify:album:5WB...|        Bob Dylan|spotify:artist:74...|     220400|Girl from the Nor...|spotify:track:4K1...| 10|              1.0|             0.0|        56706|           0.886588| 0.6773176000000001|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Blueprint 2 T...|spotify:album:6Zb...|            JAY Z|spotify:artist:3n...|     205560|  03' Bonnie & Clyde|spotify:track:73A...| 11|              0.0|             0.0|        71546|           0.856908|0.17138160000000002|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|                   8|spotify:album:6y3...|       Luis Fonsi|spotify:artist:4V...|     274213|   Que Quieres De Mi|spotify:track:3CP...| 12|              0.0|             0.0|         2893|           0.994214|0.19884280000000001|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|        Wicked Games|spotify:album:29O...|   Parra for Cuva|spotify:artist:23...|     358573|        Wicked Games|spotify:track:3mW...| 13|              0.0|             0.0|        81467|           0.837066|          0.1674132|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|       The Lumineers|spotify:album:0nR...|    The Lumineers|spotify:artist:16...|     307000|        Slow It Down|spotify:track:5OF...| 14|              0.0|             0.0|        29894|           0.940212|0.18804240000000003|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|            Greatest|spotify:album:684...|         Bee Gees|spotify:artist:1L...|     245200|How Deep Is Your ...|spotify:track:3aq...| 15|              0.0|             0.0|        31906|           0.936188|          0.1872376|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|     Cama Incendiada|spotify:album:36l...|             Man√°|spotify:artist:7o...|     272576|Mi verdad - feat....|spotify:track:3Ym...| 16|              0.0|             0.0|         4530|            0.99094|0.19818800000000003|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|Breakfast At Tiff...|spotify:album:53m...|    Henry Mancini|spotify:artist:2E...|     125585|Moon River(Vocal ...|spotify:track:5iG...| 17|              0.0|             0.0|       151521|           0.696958|          0.1393916|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|    Ultimate Sinatra|spotify:album:0Cx...|    Frank Sinatra|spotify:artist:1M...|     129813|I've Got The Worl...|spotify:track:4ts...| 18|              0.0|             0.0|       147293|           0.705414|          0.1410828|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|The Essential Elv...|spotify:album:3X3...|    Elvis Presley|spotify:artist:43...|     132013|A Little Less Con...|spotify:track:47y...| 19|              0.0|             0.0|       145093|           0.709814|          0.1419628|\n",
      "|The Times They Ar...|spotify:album:7DZ...|  Bob Dylan|spotify:artist:74...|     277106|Boots of Spanish ...|spotify:track:6QH...|  0|    For LP Fans Only|spotify:album:1NV...|    Elvis Presley|spotify:artist:43...|     115280|    That's All Right|spotify:track:0tH...| 20|              0.0|             0.0|       161826|           0.676348|          0.1352696|\n",
      "+--------------------+--------------------+-----------+--------------------+-----------+--------------------+--------------------+---+--------------------+--------------------+-----------------+--------------------+-----------+--------------------+--------------------+---+-----------------+----------------+-------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5527627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0c5edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea420804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler, Normalizer\n",
    "from pyspark.sql.functions import col, expr, round\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.stat import MultivariateGaussian\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "# Function to set up PySpark session\n",
    "def setup_spark(app_name=\"TrackSimilarity\"):\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Track_Similarity\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Function to perform feature engineering\n",
    "def feature_engineering(df):\n",
    "    # Encode categorical variables\n",
    "    album_indexer = StringIndexer(inputCol=\"album_name\", outputCol=\"album_index\")\n",
    "    artist_indexer = StringIndexer(inputCol=\"artist_name\", outputCol=\"artist_index\")\n",
    "    df = album_indexer.fit(df).transform(df)\n",
    "    df = artist_indexer.fit(df).transform(df)\n",
    "    \n",
    "    # Combine features into a vector\n",
    "    feature_cols = [\"album_index\", \"artist_index\", \"duration_ms\"]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    df = assembler.transform(df)\n",
    "    return df\n",
    "\n",
    "# Function to scale features\n",
    "def scale_features(df):\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    scaler_model = scaler.fit(df)\n",
    "    df = scaler_model.transform(df)\n",
    "    \n",
    "    # Normalize features\n",
    "    normalizer = Normalizer(inputCol=\"scaled_features\", outputCol=\"normalized_features\", p=2.0)\n",
    "    df = normalizer.transform(df)\n",
    "    # Convert normalized vector to array\n",
    "    df = df.withColumn(\"normalized_array\", vector_to_array(col(\"normalized_features\")))\n",
    "    return df\n",
    "\n",
    "# Define a UDF for the dot product\n",
    "def dot_product(vector1, vector2):\n",
    "    return float(sum(x * y for x, y in zip(vector1, vector2)))\n",
    "\n",
    "# Define a UDF for the magnitude (norm) of a vector\n",
    "def magnitude(vector):\n",
    "    return float(math.sqrt(sum(x**2 for x in vector)))\n",
    "\n",
    "# Define the final cosine similarity function\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_prod = dot_product(vector1, vector2)\n",
    "    mag1 = magnitude(vector1)\n",
    "    mag2 = magnitude(vector2)\n",
    "    \n",
    "    # To prevent division by zero, return 0 if either magnitude is zero\n",
    "    if mag1 == 0 or mag2 == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return dot_prod / (mag1 * mag2)\n",
    "\n",
    "# Register the UDF\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "# Update pairwise similarity computation\n",
    "def compute_pairwise_similarity_with_udf(df):\n",
    "    # Perform Cartesian join for pairwise comparisons\n",
    "    pairwise_df = df.alias(\"a\").crossJoin(df.alias(\"b\")).filter(col(\"a.tid\") < col(\"b.tid\"))\n",
    "    \n",
    "    # Calculate cosine similarity using the UDF\n",
    "    pairwise_df = pairwise_df.withColumn(\n",
    "        \"cosine_similarity\",\n",
    "        cosine_similarity_udf(col(\"a.normalized_array\"), col(\"b.normalized_array\"))\n",
    "    )\n",
    "    return pairwise_df\n",
    "\n",
    "def display_results(pairwise_df):\n",
    "    pairwise_df.select(\n",
    "        col(\"a.track_name\").alias(\"track_1\"),\n",
    "        col(\"b.track_name\").alias(\"track_2\"),\n",
    "        round(\"cosine_similarity\", 3).alias(\"track similarity\")\n",
    "    ).orderBy(col(\"track similarity\").asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7278054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------+----------------+\n",
      "|track_1                         |track_2      |track similarity|\n",
      "+--------------------------------+-------------+----------------+\n",
      "|Ego - Remix                     |Birthday Cake|0.01            |\n",
      "|Halo                            |Birthday Cake|0.011           |\n",
      "|If I Were a Boy                 |Birthday Cake|0.012           |\n",
      "|El Equipo Codiciado             |Birthday Cake|0.012           |\n",
      "|Ego                             |Birthday Cake|0.013           |\n",
      "|Upgrade U                       |Birthday Cake|0.013           |\n",
      "|Sobrino del Doctor Veterinario  |Birthday Cake|0.013           |\n",
      "|El Pariente                     |Birthday Cake|0.013           |\n",
      "|N1, El Perfil O El Chaval√≥n     |Birthday Cake|0.013           |\n",
      "|El Xof                          |Birthday Cake|0.014           |\n",
      "|Video Phone                     |Birthday Cake|0.015           |\n",
      "|Deja Vu                         |Birthday Cake|0.016           |\n",
      "|Diva                            |Birthday Cake|0.017           |\n",
      "|Irreplaceable                   |Birthday Cake|0.017           |\n",
      "|Marca Acme                      |Birthday Cake|0.017           |\n",
      "|El Mismo de la Clave            |Birthday Cake|0.017           |\n",
      "|Corrido del Chiquis             |Birthday Cake|0.017           |\n",
      "|1+1                             |Birthday Cake|0.018           |\n",
      "|Love On Top                     |Birthday Cake|0.018           |\n",
      "|Single Ladies (Put a Ring on It)|Birthday Cake|0.018           |\n",
      "+--------------------------------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main function to orchestrate the workflow\n",
    "def main():\n",
    "    # Step 1: Set up Spark session\n",
    "    spark = setup_spark()\n",
    "\n",
    "    # Step 2: Load data\n",
    "    df = spark.read.parquet('../data/processed/df_tracks.parquet', header=True, inferSchema=True)\n",
    "    df = df.limit(1000)\n",
    "\n",
    "    # Step 3: Feature engineering\n",
    "    df = feature_engineering(df)\n",
    "\n",
    "    # Step 4: Scale and normalize features\n",
    "    df = scale_features(df)\n",
    "\n",
    "    # Step 5: Compute pairwise similarity\n",
    "    pairwise_df = compute_pairwise_similarity_with_udf(df)\n",
    "\n",
    "    # Step 6: Display results\n",
    "    display_results(pairwise_df)\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfe5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81f612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, lit, monotonically_increasing_id\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "def create_dataframes(data_path, challenge_file, output_path, partitions):\n",
    "    \"\"\"\n",
    "    Process playlist and track data using Spark, and save the output as CSV files.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the directory containing JSON files.\n",
    "        challenge_file (str): Path to the challenge_set.json file.\n",
    "        output_path (str): Path to save the resulting CSV files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "     # Initialize SparkSession with parallelism configurations\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Create Spark DataFrames for Playlist Dataset\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions)) \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    \n",
    "    # Define column sets\n",
    "    playlist_col = ['collaborative', 'duration_ms', 'modified_at', \n",
    "                    'name', 'num_albums', 'num_artists', 'num_edits',\n",
    "                    'num_followers', 'num_tracks', 'pid']\n",
    "    tracks_col = ['album_name', 'album_uri', 'artist_name', 'artist_uri', \n",
    "                  'duration_ms', 'track_name', 'track_uri'] \n",
    "    playlist_test_col = ['name', 'num_holdouts', 'num_samples', 'num_tracks', 'pid']\n",
    "\n",
    "    # Read all JSON files in the directory\n",
    "    df_raw = spark.read.option(\"multiline\", \"true\").json(f\"{data_path}/*.json\").repartition(partitions)\n",
    "\n",
    "    # Extract playlist data\n",
    "    df_playlists = df_raw.select(explode(col(\"playlists\")).alias(\"playlist\"))\n",
    "\n",
    "    # Extract playlist-level information\n",
    "    df_playlists_info = df_playlists.select(*[col(f\"playlist.{cols}\").alias(cols) for cols in playlist_col])\n",
    "\n",
    "    # Extract track-level information\n",
    "    df_tracks = df_playlists.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        col(\"track.track_uri\").alias(\"track_uri1\"),\n",
    "        *[col(f\"track.{cols}\").alias(cols) for cols in tracks_col]\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Add unique track ID (tid) in parallel\n",
    "    df_tracks = df_tracks.withColumn(\"tid\", monotonically_increasing_id())\n",
    "\n",
    "    # Join playlist and track information to create a relationship DataFrame\n",
    "    df_playlists_tracks = df_playlists.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        col(\"pid\"),\n",
    "        col(\"track.track_uri\").alias(\"track_uri\"),\n",
    "        col(\"track.pos\").alias(\"pos\")\n",
    "    )\n",
    "    \n",
    "    df_playlists_tracks = df_playlists_tracks.join(broadcast(df_tracks), on=\"track_uri\", how=\"left\")\n",
    "    \n",
    "    # Join with track ID (tid)\n",
    "#     df_playlists_tracks = df_playlists_tracks.join(df_tracks, on=\"track_uri\", how=\"left\")\n",
    "\n",
    "    # Process challenge set\n",
    "    df_challenge_raw = spark.read.option(\"multiline\", \"true\").json(challenge_file).repartition(partitions)\n",
    "    df_playlists_test_info = df_challenge_raw.select(\n",
    "        explode(col(\"playlists\")).alias(\"playlist\")\n",
    "    ).select(*[col(f\"playlist.{cols}\").alias(cols) for cols in playlist_test_col])\n",
    "\n",
    "    df_playlists_test = df_challenge_raw.select(\n",
    "        explode(col(\"playlists\")).alias(\"playlist\")\n",
    "    ).select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        col(\"pid\"),\n",
    "        col(\"track.track_uri\").alias(\"track_uri\"),\n",
    "        col(\"track.pos\").alias(\"pos\")\n",
    "    ).join(broadcast(df_tracks), on=\"track_uri\", how=\"left\")\n",
    "\n",
    "    # Save DataFrames as CSV files\n",
    "    df_playlists_info.write.parquet(f\"{output_path}/df_playlists_info_spark\", header=True, mode=\"overwrite\")\n",
    "    df_tracks.write.parquet(f\"{output_path}/df_tracks_spark\", header=True, mode=\"overwrite\")\n",
    "    df_playlists_tracks.write.parquet(f\"{output_path}/df_playlists_spark\", header=True, mode=\"overwrite\")\n",
    "    df_playlists_test_info.write.parquet(f\"{output_path}/df_playlists_test_info_spark\", header=True, mode=\"overwrite\")\n",
    "    df_playlists_test.write.parquet(f\"{output_path}/df_playlists_test_spark\", header=True, mode=\"overwrite\")\n",
    "\n",
    "    print(\"DataFrames successfully created and saved as parquet files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    data_path = \"../data/raw/data\"  # Path to directory with JSON files\n",
    "    challenge_file = \"../data/raw/challenge_set.json\"  # Challenge set file\n",
    "    output_path = \"../data/processed/\"  # Output directory for CSV files\n",
    "     # Number of partitions for parallelism\n",
    "    num_partitions = 50  # Adjust based on your system's resources\n",
    "\n",
    "    # Run the function\n",
    "    create_dataframes(data_path, challenge_file, output_path, partitions=num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5b3cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, lit, monotonically_increasing_id\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "\n",
    "data_path = \"../data/raw/data\"\n",
    "# Initialize SparkSession with parallelism configurations\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Create Spark DataFrames for Playlist Dataset\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 200) \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    \n",
    "    # Define column sets\n",
    "playlist_col = ['collaborative', 'duration_ms', 'modified_at', \n",
    "                    'name', 'num_albums', 'num_artists', 'num_edits',\n",
    "                    'num_followers', 'num_tracks', 'pid']\n",
    "tracks_col = ['album_name', 'album_uri', 'artist_name', 'artist_uri', \n",
    "                  'duration_ms', 'track_name', 'track_uri'] \n",
    "playlist_test_col = ['name', 'num_holdouts', 'num_samples', 'num_tracks', 'pid']\n",
    "\n",
    "    # Read all JSON files in the directory\n",
    "df_raw = spark.read.option(\"multiline\", \"true\").json(f\"{data_path}/*.json\").repartition(200)\n",
    "\n",
    "    # Extract playlist data\n",
    "df_playlists = df_raw.select(explode(col(\"playlists\")).alias(\"playlist\"))\n",
    "\n",
    "    # Extract playlist-level information\n",
    "df_playlists_info = df_playlists.select(*[col(f\"playlist.{cols}\").alias(cols) for cols in playlist_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d4d22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Extract track-level information\n",
    "df_tracks = df_playlists.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        *[col(f\"track.{cols}\").alias(cols) for cols in tracks_col]\n",
    "    ).drop_duplicates()\n",
    "\n",
    "df_tracks = df_tracks.withColumn(\"tid\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0eedff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playlist: struct (nullable = true)\n",
      " |    |-- collaborative: string (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- duration_ms: long (nullable = true)\n",
      " |    |-- modified_at: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- num_albums: long (nullable = true)\n",
      " |    |-- num_artists: long (nullable = true)\n",
      " |    |-- num_edits: long (nullable = true)\n",
      " |    |-- num_followers: long (nullable = true)\n",
      " |    |-- num_tracks: long (nullable = true)\n",
      " |    |-- pid: long (nullable = true)\n",
      " |    |-- tracks: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- album_name: string (nullable = true)\n",
      " |    |    |    |-- album_uri: string (nullable = true)\n",
      " |    |    |    |-- artist_name: string (nullable = true)\n",
      " |    |    |    |-- artist_uri: string (nullable = true)\n",
      " |    |    |    |-- duration_ms: long (nullable = true)\n",
      " |    |    |    |-- pos: long (nullable = true)\n",
      " |    |    |    |-- track_name: string (nullable = true)\n",
      " |    |    |    |-- track_uri: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_playlists.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c861a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_uri: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_uri: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- track_uri: string (nullable = true)\n",
      " |-- tid: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tracks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78a273e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[FIELD_NOT_FOUND] No such struct field `tid` in `album_name`, `album_uri`, `artist_name`, `artist_uri`, `duration_ms`, `pos`, `track_name`, `track_uri`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m df_tracks \u001b[38;5;241m=\u001b[39m df_tracks\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtid\u001b[39m\u001b[38;5;124m\"\u001b[39m, monotonically_increasing_id())\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Join playlist and track information to create a relationship DataFrame\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_playlists_tracks \u001b[38;5;241m=\u001b[39m \u001b[43mdf_playlists\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplaylist.pid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplaylist.tracks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrack\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrack.tid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrack.pos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:3229\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m     \u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3186\u001b[0m \n\u001b[1;32m   3187\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3229\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [FIELD_NOT_FOUND] No such struct field `tid` in `album_name`, `album_uri`, `artist_name`, `artist_uri`, `duration_ms`, `pos`, `track_name`, `track_uri`."
     ]
    }
   ],
   "source": [
    "# Add unique track ID (tid) in parallel\n",
    "df_tracks = df_tracks.withColumn(\"tid\", monotonically_increasing_id())\n",
    "\n",
    "    # Join playlist and track information to create a relationship DataFrame\n",
    "df_playlists_tracks = df_playlists.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        col(\"pid\"),\n",
    "        col(\"track.tid\").alias(\"tid\"),\n",
    "        col(\"track.pos\").alias(\"pos\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2bcdd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pid: long (nullable = true)\n",
      " |-- pos: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_playlists_tracks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915b0bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5bf26a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- track_uri1: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_uri: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_uri: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- track_uri: string (nullable = true)\n",
      " |-- tid: long (nullable = false)\n",
      "\n",
      "track schema:None\n",
      "root\n",
      " |-- collaborative: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- modified_at: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_albums: long (nullable = true)\n",
      " |-- num_artists: long (nullable = true)\n",
      " |-- num_edits: long (nullable = true)\n",
      " |-- num_followers: long (nullable = true)\n",
      " |-- num_tracks: long (nullable = true)\n",
      " |-- pid: long (nullable = true)\n",
      "\n",
      "playlist schema:None\n",
      "root\n",
      " |-- track_uri: string (nullable = true)\n",
      " |-- pid: long (nullable = true)\n",
      " |-- pos: long (nullable = true)\n",
      " |-- track_uri1: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_uri: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_uri: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- tid: long (nullable = true)\n",
      "\n",
      "playlist track schema:None\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_holdouts: long (nullable = true)\n",
      " |-- num_samples: long (nullable = true)\n",
      " |-- num_tracks: long (nullable = true)\n",
      " |-- pid: long (nullable = true)\n",
      "\n",
      "playlist test info schema:None\n",
      "root\n",
      " |-- track_uri: string (nullable = true)\n",
      " |-- pid: long (nullable = true)\n",
      " |-- pos: long (nullable = true)\n",
      " |-- track_uri1: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_uri: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_uri: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- tid: long (nullable = true)\n",
      "\n",
      "playlist test schema:None\n",
      "DataFrames successfully created and saved as parquet files.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, lit, monotonically_increasing_id\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "def create_dataframes(data_path, challenge_file, output_path, partitions):\n",
    "    \"\"\"\n",
    "    Process playlist and track data using Spark, and save the output as CSV files.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the directory containing JSON files.\n",
    "        challenge_file (str): Path to the challenge_set.json file.\n",
    "        output_path (str): Path to save the resulting CSV files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "     # Initialize SparkSession with parallelism configurations\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Create Spark DataFrames for Playlist Dataset\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions)) \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    \n",
    "    # Define column sets\n",
    "    playlist_col = ['collaborative', 'duration_ms', 'modified_at', \n",
    "                    'name', 'num_albums', 'num_artists', 'num_edits',\n",
    "                    'num_followers', 'num_tracks', 'pid']\n",
    "    tracks_col = ['album_name', 'album_uri', 'artist_name', 'artist_uri', \n",
    "                  'duration_ms', 'track_name', 'track_uri'] \n",
    "    playlist_test_col = ['name', 'num_holdouts', 'num_samples', 'num_tracks', 'pid']\n",
    "\n",
    "    # Read all JSON files in the directory\n",
    "    df_raw = spark.read.option(\"multiline\", \"true\").json(f\"{data_path}/*.json\").repartition(partitions)\n",
    "\n",
    "    # Extract playlist data\n",
    "    df_playlists = df_raw.select(explode(col(\"playlists\")).alias(\"playlist\"))\n",
    "\n",
    "    # Extract playlist-level information\n",
    "    df_playlists_info = df_playlists.select(*[col(f\"playlist.{cols}\").alias(cols) for cols in playlist_col])\n",
    "\n",
    "    # Extract track-level information\n",
    "    df_tracks = df_playlists.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        col(\"track.track_uri\").alias(\"track_uri1\"),\n",
    "        *[col(f\"track.{cols}\").alias(cols) for cols in tracks_col]\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Add unique track ID (tid) in parallel\n",
    "    df_tracks = df_tracks.withColumn(\"tid\", monotonically_increasing_id())\n",
    "\n",
    "    # Join playlist and track information to create a relationship DataFrame\n",
    "    df_playlists_tracks = df_playlists.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        col(\"pid\"),\n",
    "        col(\"track.track_uri\").alias(\"track_uri\"),\n",
    "        col(\"track.pos\").alias(\"pos\")\n",
    "    )\n",
    "    \n",
    "    df_playlists_tracks = df_playlists_tracks.join(broadcast(df_tracks), on=\"track_uri\", how=\"left\")\n",
    "    \n",
    "    # Join with track ID (tid)\n",
    "#     df_playlists_tracks = df_playlists_tracks.join(df_tracks, on=\"track_uri\", how=\"left\")\n",
    "\n",
    "    # Process challenge set\n",
    "    df_challenge_raw = spark.read.option(\"multiline\", \"true\").json(challenge_file).repartition(partitions)\n",
    "    df_playlists_test_info = df_challenge_raw.select(\n",
    "        explode(col(\"playlists\")).alias(\"playlist\")\n",
    "    ).select(*[col(f\"playlist.{cols}\").alias(cols) for cols in playlist_test_col])\n",
    "\n",
    "    df_playlists_test = df_challenge_raw.select(\n",
    "        explode(col(\"playlists\")).alias(\"playlist\")\n",
    "    ).select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        col(\"pid\"),\n",
    "        col(\"track.track_uri\").alias(\"track_uri\"),\n",
    "        col(\"track.pos\").alias(\"pos\")\n",
    "    ).join(broadcast(df_tracks), on=\"track_uri\", how=\"left\")\n",
    "    \n",
    "    print(\"track schema:{}\".format(df_tracks.printSchema()))\n",
    "    print(\"playlist schema:{}\".format(df_playlists_info.printSchema()))\n",
    "    print(\"playlist track schema:{}\".format(df_playlists_tracks.printSchema()))\n",
    "    print(\"playlist test info schema:{}\".format(df_playlists_test_info.printSchema()))\n",
    "    print(\"playlist test schema:{}\".format(df_playlists_test.printSchema()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     # Save DataFrames as CSV files\n",
    "#     df_playlists_info.write.parquet(f\"{output_path}/df_playlists_info_spark\", header=True, mode=\"overwrite\")\n",
    "#     df_tracks.write.parquet(f\"{output_path}/df_tracks_spark\", header=True, mode=\"overwrite\")\n",
    "#     df_playlists_tracks.write.parquet(f\"{output_path}/df_playlists_spark\", header=True, mode=\"overwrite\")\n",
    "#     df_playlists_test_info.write.parquet(f\"{output_path}/df_playlists_test_info_spark\", header=True, mode=\"overwrite\")\n",
    "#     df_playlists_test.write.parquet(f\"{output_path}/df_playlists_test_spark\", header=True, mode=\"overwrite\")\n",
    "\n",
    "    print(\"DataFrames successfully created and saved as parquet files.\")\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    data_path = \"../data/raw/data\"  # Path to directory with JSON files\n",
    "    challenge_file = \"../data/raw/challenge_set.json\"  # Challenge set file\n",
    "    output_path = \"../data/processed/\"  # Output directory for CSV files\n",
    "     # Number of partitions for parallelism\n",
    "    num_partitions = 50  # Adjust based on your system's resources\n",
    "\n",
    "    # Run the function\n",
    "    create_dataframes(data_path, challenge_file, output_path, partitions=num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4169fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- collaborative: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- modified_at: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- num_albums: long (nullable = true)\n",
      " |-- num_artists: long (nullable = true)\n",
      " |-- num_edits: long (nullable = true)\n",
      " |-- num_followers: long (nullable = true)\n",
      " |-- num_tracks: long (nullable = true)\n",
      " |-- pid: long (nullable = true)\n",
      "\n",
      "playlist_schema:None\n",
      "root\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_uri: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_uri: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- track_uri: string (nullable = true)\n",
      " |-- tid: long (nullable = false)\n",
      "\n",
      "playlist_schema:None\n",
      "root\n",
      " |-- track_uri: string (nullable = true)\n",
      " |-- pid: long (nullable = true)\n",
      " |-- pos: long (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- album_uri: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_uri: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- tid: long (nullable = true)\n",
      "\n",
      "playlist_schema:None\n",
      "DataFrames successfully created and saved as parquet files.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, lit, monotonically_increasing_id\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "\n",
    "def create_dataframes(data_path, challenge_file, output_path, partitions):\n",
    "    \"\"\"\n",
    "    Process playlist and track data using Spark, and save the output as CSV files.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the directory containing JSON files.\n",
    "        challenge_file (str): Path to the challenge_set.json file.\n",
    "        output_path (str): Path to save the resulting CSV files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize SparkSession with parallelism configurations, 4 cores, 4GB memory\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Create Spark DataFrames for Playlist Dataset\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions)) \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Djava.security.manager=allow\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Define column sets\n",
    "    playlist_col = ['collaborative', 'duration_ms', 'modified_at',\n",
    "                    'name', 'num_albums', 'num_artists', 'num_edits',\n",
    "                    'num_followers', 'num_tracks', 'pid']\n",
    "    tracks_col = ['album_name', 'album_uri', 'artist_name', 'artist_uri',\n",
    "                  'duration_ms', 'track_name', 'track_uri']\n",
    "    playlist_test_col = ['name', 'pid']\n",
    "\n",
    "    # Read all JSON files in the directory\n",
    "    df_raw = spark.read.option(\"multiline\", \"true\").json(f\"{data_path}/*.json\").repartition(partitions)\n",
    "\n",
    "    # Extract playlist data\n",
    "    df_playlists = df_raw.select(explode(col(\"playlists\")).alias(\"playlist\"))\n",
    "\n",
    "    # Extract playlist-level information\n",
    "    df_playlists_info = df_playlists.select(*[col(f\"playlist.{cols}\").alias(cols) for cols in playlist_col])\n",
    "\n",
    "    # Extract track-level information\n",
    "    df_tracks = df_playlists.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        *[col(f\"track.{cols}\").alias(cols) for cols in tracks_col]\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    # Add unique track ID (tid) in parallel\n",
    "    df_tracks = df_tracks.withColumn(\"tid\", monotonically_increasing_id())\n",
    "\n",
    "    # Join playlist and track information to create a relationship DataFrame\n",
    "    df_playlists_tracks = df_playlists.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        col(\"pid\"),\n",
    "        col(\"track.track_uri\").alias(\"track_uri\"),\n",
    "        col(\"track.pos\").alias(\"pos\")\n",
    "    )\n",
    "\n",
    "    df_playlists_tracks = df_playlists_tracks.join(broadcast(df_tracks), on=\"track_uri\", how=\"left\")\n",
    "\n",
    "    # Join with track ID (tid)\n",
    "    #     df_playlists_tracks = df_playlists_tracks.join(df_tracks, on=\"track_uri\", how=\"left\")\n",
    "\n",
    "    # Process challenge set\n",
    "    df_challenge_raw = spark.read.option(\"multiline\", \"true\").json(challenge_file).repartition(partitions)\n",
    "\n",
    "    # Extract playlist data\n",
    "    df_playlists_challenge = df_challenge_raw.select(explode(col(\"playlists\")).alias(\"playlist\"))\n",
    "\n",
    "    # Extract playlist-level information\n",
    "    df_playlists_challenge_info = df_playlists_challenge.select(*[col(f\"playlist.{cols}\").alias(cols) for cols in playlist_test_col])\n",
    "#     df_playlists_challenge_info.printSchema()\n",
    "\n",
    "    # Extract track-level information\n",
    "    df_tracks_challenge = df_playlists_challenge.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        *[col(f\"track.{cols}\").alias(cols) for cols in tracks_col]\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    # Add unique track ID (tid) in parallel\n",
    "    df_tracks_challenge = df_tracks_challenge.withColumn(\"tid\", monotonically_increasing_id())\n",
    "#     df_tracks_challenge.printSchema()\n",
    "\n",
    "    # Join playlist and track information to create a relationship DataFrame\n",
    "    df_playlists_tracks_challenge = df_playlists_challenge.select(\n",
    "        col(\"playlist.pid\").alias(\"pid\"),\n",
    "        explode(col(\"playlist.tracks\")).alias(\"track\")\n",
    "    ).select(\n",
    "        col(\"pid\"),\n",
    "        col(\"track.track_uri\").alias(\"track_uri\"),\n",
    "        col(\"track.pos\").alias(\"pos\")\n",
    "    )\n",
    "    \n",
    "    print(\"playlist_schema:{}\".format(df_playlists_info.printSchema()))\n",
    "    print(\"playlist_schema:{}\".format(df_tracks.printSchema()))\n",
    "    print(\"playlist_schema:{}\".format(df_playlists_tracks.printSchema()))\n",
    "          \n",
    "    df_playlists_tracks_challenge = df_playlists_tracks_challenge.join(broadcast(df_tracks_challenge), on=\"track_uri\", how=\"left\")\n",
    "#     df_playlists_tracks_challenge.printSchema()\n",
    "\n",
    "    \n",
    "\n",
    "#     # Save DataFrames as parquet files\n",
    "#     df_playlists_info.write.parquet(f\"{output_path}/df_playlists_info_spark\", mode=\"overwrite\")\n",
    "#     df_tracks.write.parquet(f\"{output_path}/df_tracks_spark\", mode=\"overwrite\")\n",
    "#     df_playlists_tracks.write.parquet(f\"{output_path}/df_playlists_spark\", mode=\"overwrite\")\n",
    "\n",
    "#     # Save challenge DataFrames as parquet files\n",
    "#     df_playlists_challenge_info.write.parquet(f\"{output_path}/df_playlists_test_info_spark\", mode=\"overwrite\")\n",
    "#     df_tracks_challenge.write.parquet(f\"{output_path}/df_tracks_test_spark\", mode=\"overwrite\")\n",
    "#     df_playlists_tracks_challenge.write.parquet(f\"{output_path}/df_playlists_test_spark\", mode=\"overwrite\")\n",
    "\n",
    "    print(\"DataFrames successfully created and saved as parquet files.\")\n",
    "\n",
    "    # Stop the SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    data_path = \"../data/raw/data\"  # Path to directory with JSON files\n",
    "    challenge_file = \"../data/raw/challenge_set.json\"  # Challenge set file\n",
    "    output_path = \"../data/processed/\"  # Output directory for CSV files\n",
    "    # Number of partitions for parallelism\n",
    "    num_partitions = 50  # Adjust based on your system's resources\n",
    "\n",
    "    # Run the function\n",
    "    create_dataframes(data_path, challenge_file, output_path, partitions=num_partitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0134e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
